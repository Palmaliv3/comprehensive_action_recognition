{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (Input, ConvLSTM2D, MaxPooling3D, \n",
    "                                   Dropout, Dense, GlobalAveragePooling3D,\n",
    "                                   TimeDistributed, MultiHeadAttention,\n",
    "                                   Reshape, GlobalAveragePooling1D)\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical, Sequence\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau, \n",
    "                                      ModelCheckpoint, TerminateOnNaN)\n",
    "from tensorflow.keras import mixed_precision\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import torch\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from classes import CLASSES_LIST, IMAGE_HEIGHT, IMAGE_WIDTH, SEQUENCE_LENGTH, BATCH_SIZE, DATASET_DIR\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store weights using float32 (accuracy)\n",
    "# Make computations using float16 (effectiveness)\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "# Set it up for all layers in the model\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Load pre-trained model for features extraction\n",
    "base_model = EfficientNetB0(weights=\"imagenet\", include_top=False, \n",
    "                           pooling=\"avg\", input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=base_model.output).to(device)  \n",
    "\n",
    "\"\"\"\n",
    "EfficientNetB0 is a pre-trained CNN model for recognising actions on images, \n",
    "trained on the ImageNet dataset, containing around 1000 categories cover a wide range of objects,\n",
    "including animals, vehicles, household items, and more.\n",
    "I prefered to choose this model because I needed a high accuracy with relatively low computational cost.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_frame_extraction(video_path):\n",
    "    # List for frames extraction\n",
    "    frames = []\n",
    "\n",
    "    # Capture video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get the overall number of frames\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    if total_frames <= SEQUENCE_LENGTH:\n",
    "        # if number of frames <= required number,\n",
    "        # just take all of them.\n",
    "        indices = range(total_frames)\n",
    "    else:\n",
    "        # if number of frames > required number,\n",
    "        # concentrate on the central part.\n",
    "        # We can just skip some parts of the video, \n",
    "        # but it would be uneffective for training as there are gonna be a lot of unnecessary data.\n",
    "        middle = total_frames // 2\n",
    "        radius = SEQUENCE_LENGTH // 2\n",
    "\n",
    "        # Take consecutive frames around the center by index\n",
    "        indices = range(max(0, middle-radius), min(total_frames, middle+radius))\n",
    "\n",
    "        # If the number of frames still not enough,\n",
    "        # randomly select the SEQUENCE_LENGTH of frames\n",
    "        if len(indices) < SEQUENCE_LENGTH:\n",
    "            indices = sorted(random.sample(range(total_frames), SEQUENCE_LENGTH))\n",
    "\n",
    "    # Frames processing \n",
    "    for idx in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)  # set the position on a frame by index\n",
    "        ret, frame = cap.read()  # Read frame\n",
    "        if ret:\n",
    "            frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))  # Change the size\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Transform a color format from BGR to RGB \n",
    "            frames.append(frame)  # Append to the list\n",
    "\n",
    "\n",
    "    while len(frames) < SEQUENCE_LENGTH:\n",
    "        # If not enough frames, just copy the last one and flip it \n",
    "        frames.append(np.flip(frames[-1], axis=0))\n",
    "    \"\"\"\n",
    "    P.s. Its possible to add this step to overall training and slightly improve the accurasy,\n",
    "    but here I decided not to do it because it could lead to overtraining.\n",
    "    \"\"\"\n",
    "\n",
    "    # Release the captioned resources\n",
    "    cap.release()\n",
    "\n",
    "    # Return the frames array using float16 data type\n",
    "    return np.array(frames, dtype=np.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_cache_features():\n",
    "    # Iterate over each class in the list\n",
    "    for class_idx, class_name in enumerate(CLASSES_LIST):\n",
    "        # Connect path of a class with the video \n",
    "        class_dir = os.path.join(DATASET_DIR, class_name)\n",
    "\n",
    "        # Path, where features gonna be stored\n",
    "        save_dir = f\"features_2\\\\{class_name}\"\n",
    "        os.makedirs(save_dir, exist_ok=True)  \n",
    "\n",
    "        # Iterate over each video in the class folder\n",
    "        for video_file in os.listdir(class_dir):\n",
    "            video_path = os.path.join(class_dir, video_file)  # full path to the video\n",
    "\n",
    "            # Extract frames from a previously defined function \n",
    "            frames = smart_frame_extraction(video_path)\n",
    "\n",
    "            # Extract features from a previously defined model \n",
    "            features = feature_extractor.predict(frames, verbose=0)\n",
    "\n",
    "            # Transform the format to float16 for the efficient storage handling\n",
    "            features = features.astype(np.float16)\n",
    "\n",
    "            # Check: if features extracted right\n",
    "            if features.shape == (SEQUENCE_LENGTH, 1280):\n",
    "                print(f\"@ The form of the extracted features: {features.shape} for video {video_file}\")\n",
    "\n",
    "                # Path for saving a file in .npy format\n",
    "                save_path = os.path.join(save_dir, f\"{video_file}.npy\")\n",
    "\n",
    "                np.save(save_path, features)\n",
    "                print(f\"-- Features, saved for video: {video_file}\")\n",
    "\n",
    "            else:\n",
    "                # Warnings\n",
    "                print(f\"! Warning: extracted objects do not match the shape {features.shape} for {video_file}\\n\")\n",
    "                print(f\"! Warning: not suitable number of frames {len(frames)} in {video_file}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "extract_and_cache_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
