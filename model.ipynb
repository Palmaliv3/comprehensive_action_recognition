{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.utils import to_categorical, Sequence\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau, \n",
    "                                      ModelCheckpoint, TerminateOnNaN)\n",
    "from tensorflow.keras.layers import (Input, ConvLSTM2D, MaxPooling3D, \n",
    "                                   Dropout, Dense, GlobalAveragePooling3D,\n",
    "                                   TimeDistributed, MultiHeadAttention,\n",
    "                                   Reshape, GlobalMaxPooling1D,LayerNormalization)\n",
    "\n",
    "import tensorflow as tf\n",
    "from classes import CLASSES_LIST, IMAGE_HEIGHT, IMAGE_WIDTH, SEQUENCE_LENGTH, BATCH_SIZE, DATASET_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Configuration for TensorFlow (Optimized for GTX 1650 Max-Q)\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Configure GPU memory growth to avoid OOM errors on 4GB VRAM\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable memory growth for GTX 1650 Max-Q\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        # Set memory limit to ~3.5GB to leave room for system\n",
    "        tf.config.experimental.set_memory_limit(gpus[0], 3584)  # 3.5GB in MB\n",
    "        print(f\"Using GTX 1650 Max-Q - Memory limit set to 3.5GB\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "        print(\"Falling back to default configuration\")\n",
    "else:\n",
    "    print(\"No GPU found, using CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved features\n",
    "def load_features():\n",
    "    features, labels = [], []\n",
    "    for class_idx, class_name in enumerate(CLASSES_LIST):\n",
    "    # enumerate - a function for iterating through the elements and returning them along with their indexes \n",
    "        feature_dir = os.path.join(\"features\", class_name)\n",
    "        for feature_file in os.listdir(feature_dir):\n",
    "            feature_path = os.path.join(feature_dir, feature_file)\n",
    "            feat = np.load(feature_path, mmap_mode='r')\n",
    "            # mmap_mode - a parameter telling to use momory-mapped mode for uploading data\n",
    "            # saving memory\n",
    "            features.append(feat)\n",
    "            labels.append(class_idx)\n",
    "    \n",
    "    return np.array(features), to_categorical(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used functional model style for compatibility with MultiHeadAttention,\n",
    "# which allows a model to focus on different parts of the input sequence simultaneously,\n",
    "# capturing multiple types of relationships. \n",
    "\n",
    "def build_hybrid_model():\n",
    "    # Input layer: features from EfficientNetB0\n",
    "    inputs = Input(shape=(SEQUENCE_LENGTH, 1280), name='input_layer')\n",
    "    \n",
    "    # Transform the entry form: (SEQUENCE_LENGTH, 1, 1, 1280)\n",
    "    x = Reshape((SEQUENCE_LENGTH, 1, 1, 1280), name='reshape_for_lstm')(inputs)\n",
    "\n",
    "    # ConvLSTM2D layer to capture time dependencies\n",
    "    x = ConvLSTM2D(\n",
    "        filters=70,\n",
    "        kernel_size=(1, 1),\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid',\n",
    "        recurrent_dropout=0.2,\n",
    "        return_sequences=True,\n",
    "        name='convlstm2d'\n",
    "    )(x)\n",
    "\n",
    "    # Unfolding spatial dimensions before the attention method\n",
    "    x = Reshape((SEQUENCE_LENGTH, -1), name='reshape_for_attention')(x)\n",
    "\n",
    "    # Normalizes each token’s embedding vector across its features.\n",
    "    # Prevents exploding/vanishing activations and prepares for the attention layer.\n",
    "    norm_x = LayerNormalization(name='pre_attn_norm')(x)\n",
    "\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=4,\n",
    "        key_dim=64,\n",
    "        name='multihead_attention'\n",
    "    )(norm_x, norm_x)  # query = key = value = norm_x\n",
    "    # num_heads=4 - number of parallel heads of attention\n",
    "    # key_dim=128 - The dimension of the vectors of the key (Key) and query (Query) for each head. Affects the detail of attention\n",
    "    \"\"\"\n",
    "        Query searches for connections with Key.\n",
    "        Value returns the context for the Query(for example, that the action is related to something)\n",
    "    \"\"\"\n",
    "\n",
    "    # Dropout for regularization after attention\n",
    "    x = Dropout(0.2, name='dropout_after_attention')(attention_output)\n",
    "\n",
    "    # Averaging along the time axis\n",
    "    x = GlobalMaxPooling1D(name='global_avg_pool')(x)\n",
    "\n",
    "    # Fully connected layer with 'ReLU' activation\n",
    "    x = Dense(128, activation='relu', name='dense_relu')(x)\n",
    "\n",
    "    # Dropout after a fully connected layer\n",
    "    x = Dropout(0.4, name='dropout_after_dense')(x)\n",
    "\n",
    "    # Output layer with 'softmax' for classification\n",
    "    outputs = Dense(len(CLASSES_LIST), activation='softmax', dtype='float32', name='output_softmax')(x)\n",
    "\n",
    "    # Building a model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='Hybrid_Attention_Model')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class implements a batch generator \n",
    "# with the ability to augment data \"on the fly\" during model training,\n",
    "# to avoid overfitting.\n",
    "# Still need to use this method very carefully.\n",
    "\n",
    "class AugmentedDataGenerator(Sequence):\n",
    "# Sequence -  makes our generator compatible with model.fit() and it allows you to use multithreading.\n",
    "    \n",
    "    def __init__(self, x_data, y_data, batch_size, augment=False):\n",
    "        # Save the input data and settings\n",
    "        self.x = x_data                     \n",
    "        self.y = y_data                     \n",
    "        self.batch_size = batch_size        \n",
    "        self.augment = augment \n",
    "        self.indices = np.arange(len(x_data))\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the number of batches per epoch from the entire dataset to be generated.\n",
    "        # ceil - rounding it up to a higher value\n",
    "        return int(np.ceil(len(self.x) / self.batch_size))\n",
    "        # Divides the amount of data (len(self.x)) by the size of the batch (self.batch_size).\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "    # This method is called when requesting the next batch during training.\n",
    "\n",
    "        batch_indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]  # Pulling out the indexes of the current batch\n",
    "        batch_x = self.x[batch_indices]   # selection of features by indexes\n",
    "        batch_y = self.y[batch_indices]   # selection of class labels by indexes\n",
    "        \n",
    "        if self.augment:\n",
    "            batch_x = self._augment_batch(batch_x)  # apply augmentation if enabled\n",
    "            \n",
    "        return batch_x, batch_y  # returns finished batch (X and y)\n",
    "\n",
    "    def _augment_batch(self, batch):\n",
    "        # Performs data augmentation,\n",
    "        # for example, randomly flips sequences \n",
    "        # (in this case, along the time axis).\n",
    "        augmented = []\n",
    "        for seq in batch:\n",
    "            if random.random() > 0.5:\n",
    "            # Выбираем рандомно\n",
    "                seq = np.flip(seq, axis=0)  # Временной разворот\n",
    "            augmented.append(seq)\n",
    "        return np.array(augmented)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    print(\"Loading features...\")\n",
    "    features, labels = load_features()\n",
    "    # Split into training and test parts.\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=0.1, stratify=labels, random_state=42)\n",
    "    \n",
    "    # Build model\n",
    "    with tf.device('/GPU:0'):\n",
    "        model = build_hybrid_model()\n",
    "    optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),\n",
    "        # If val_accuracy does not increase for 6 epochs, then the training process stops\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6),\n",
    "        # The same item as EarlyStopping, but for val_loss, only here learning_rate is reduced by multiplying by 'factor'.\n",
    "        # Prevents overfitting\n",
    "        TerminateOnNaN() # Stop training at one of the parameters = NaN\n",
    "    ]\n",
    "    \n",
    "    # Data augmentation\n",
    "    train_gen = AugmentedDataGenerator(x_train, y_train, BATCH_SIZE, augment=True)\n",
    "    val_gen = AugmentedDataGenerator(x_test, y_test, BATCH_SIZE)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        epochs=50,\n",
    "        validation_data=val_gen,\n",
    "        # tests\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "        # verbose - shows 1 training scale for each epoch\n",
    "    )\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading a model for further training, if it exists\n",
    "# or\n",
    "# Creating it\n",
    "if os.path.exists(\"trained_model_.keras\"):\n",
    "    model = load_model(\"trained_model_.keras\")\n",
    "    print(\"The existing model has been loaded\")\n",
    "else:\n",
    "    model, history = train_model()\n",
    "\n",
    "    # Building charts\n",
    "    train_acc = history.history['accuracy']\n",
    "    test_acc = history.history['val_accuracy']\n",
    "    train_loss = history.history['loss']\n",
    "    test_loss = history.history['val_loss']\n",
    "\n",
    "    def plot_metric(metric1, metric2, plot_n):\n",
    "        if len(metric1) != len(metric2):\n",
    "            print(f\"! Warning: The metric lengths do not match! Metric1: {len(metric1)}, Metric2: {len(metric2)}\\n\")\n",
    "            min_len = min(len(metric1), len(metric2))\n",
    "            metric1 = metric1[:min_len]\n",
    "            metric2 = metric2[:min_len]\n",
    "            \n",
    "        epochs = range(len(metric1))\n",
    "\n",
    "        plt.plot(epochs, metric1, 'r', label=f'{plot_n} (train)')\n",
    "        plt.plot(epochs, metric2, 'b', label=f'{plot_n} (validation)')\n",
    "        plt.title(f'Training and Validation {plot_n}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    plot_metric(train_acc, test_acc, 'Accuracy')\n",
    "    plot_metric(train_loss, test_loss, 'Loss')\n",
    "\n",
    "\n",
    "    model.save(\"trained_model_.keras\")\n",
    "    print(\"№ The model was successfully created and saved\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
